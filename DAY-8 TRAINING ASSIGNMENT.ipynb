{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3pIyKTBxh+jd9k2cEAax+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksharasugumanchi/ExcelR/blob/main/DAY-8%20TRAINING%20ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python script that:\n",
        "1. Tokenizes a sample paragraph into words and sentences.\n",
        "\n"
      ],
      "metadata": {
        "id": "euc8Hoybi7YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty2rGHPyqRls",
        "outputId": "cc4f8688-04c9-46a3-f77d-64991f2126ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download necessary resources from NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample paragraph\n",
        "paragraph = \"Tokenization is an essential part of natural language processing. It involves breaking down text into smaller units, such as sentences or words. This makes it easier to analyze and process text data.\"\n",
        "\n",
        "# Tokenize into sentences\n",
        "sentences = sent_tokenize(paragraph)\n",
        "print(\"Sentences:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}: {sentence}\")\n",
        "\n",
        "# Tokenize into words\n",
        "words = word_tokenize(paragraph)\n",
        "print(\"\\nWords:\")\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc_98XElqGZM",
        "outputId": "f8c76b8c-d21a-40f1-8bca-e47dc2a4843e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "1: Tokenization is an essential part of natural language processing.\n",
            "2: It involves breaking down text into smaller units, such as sentences or words.\n",
            "3: This makes it easier to analyze and process text data.\n",
            "\n",
            "Words:\n",
            "['Tokenization', 'is', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'It', 'involves', 'breaking', 'down', 'text', 'into', 'smaller', 'units', ',', 'such', 'as', 'sentences', 'or', 'words', '.', 'This', 'makes', 'it', 'easier', 'to', 'analyze', 'and', 'process', 'text', 'data', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}