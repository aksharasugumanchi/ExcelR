{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnlwAKcZRY/OXz0I0PtXBa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksharasugumanchi/ExcelR/blob/main/DAY-13%20TRAINING%20ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python script to perform part-of-speech tagging on the sentence: 'NLP is amazing and fun to learn.' using SpaCy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "euc8Hoybi7YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty2rGHPyqRls",
        "outputId": "cc4f8688-04c9-46a3-f77d-64991f2126ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "\n",
        "# Download necessary resources from NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the given text by removing special characters and converting to lowercase.\n",
        "    :param text: str, input text to clean\n",
        "    :return: str, cleaned text\n",
        "    \"\"\"\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
        "    return cleaned_text.lower()  # Convert to lowercase\n",
        "\n",
        "def extract_emails(text):\n",
        "    \"\"\"\n",
        "    Extracts all email addresses from the given text.\n",
        "    :param text: str, input text containing email addresses\n",
        "    :return: list, extracted email addresses\n",
        "    \"\"\"\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "    return re.findall(email_pattern, text)\n",
        "\n",
        "def fetch_webpage_title(url):\n",
        "    \"\"\"\n",
        "    Fetches and returns the title of a webpage.\n",
        "    :param url: str, URL of the webpage\n",
        "    :return: str, title of the webpage\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return soup.title.string.strip() if soup.title else \"No title found\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching the URL: {e}\"\n",
        "\n",
        "def generate_wordcloud(text, output_file=\"wordcloud.png\"):\n",
        "    \"\"\"\n",
        "    Generates a WordCloud from the given text and saves it as an image.\n",
        "    :param text: str, input text to generate the WordCloud\n",
        "    :param output_file: str, file name to save the WordCloud image\n",
        "    \"\"\"\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    wordcloud.to_file(output_file)\n",
        "    print(f\"WordCloud saved as {output_file}\")\n",
        "\n",
        "def pos_tagging(sentence):\n",
        "    \"\"\"\n",
        "    Performs part-of-speech tagging on the given sentence using SpaCy.\n",
        "    :param sentence: str, input sentence to tag\n",
        "    :return: list of tuples (word, POS tag)\n",
        "    \"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "# Sample paragraph\n",
        "paragraph = \"Tokenization is an essential part of natural language processing. It involves breaking down text into smaller units, such as sentences or words. This makes it easier to analyze and process text data.\"\n",
        "\n",
        "# Tokenize into sentences\n",
        "sentences = sent_tokenize(paragraph)\n",
        "print(\"Sentences:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}: {sentence}\")\n",
        "\n",
        "# Tokenize into words\n",
        "words = word_tokenize(paragraph)\n",
        "print(\"\\nWords:\")\n",
        "print(words)\n",
        "\n",
        "# Test clean_text function\n",
        "test_input = 'Hello, World! Welcome to NLP 101.'\n",
        "cleaned_output = clean_text(test_input)\n",
        "print(\"\\nCleaned Text:\")\n",
        "print(cleaned_output)\n",
        "\n",
        "# Test extract_emails function\n",
        "test_email_input = 'Contact us at support@example.com and sales@example.org.'\n",
        "extracted_emails = extract_emails(test_email_input)\n",
        "print(\"\\nExtracted Emails:\")\n",
        "print(extracted_emails)\n",
        "\n",
        "# Test fetch_webpage_title function\n",
        "test_url = 'https://example.com'\n",
        "webpage_title = fetch_webpage_title(test_url)\n",
        "print(\"\\nWebpage Title:\")\n",
        "print(webpage_title)\n",
        "\n",
        "# Test generate_wordcloud function\n",
        "wordcloud_text = 'data science machine learning artificial intelligence'\n",
        "generate_wordcloud(wordcloud_text)\n",
        "\n",
        "# Test pos_tagging function\n",
        "test_sentence = 'NLP is amazing and fun to learn.'\n",
        "pos_tags = pos_tagging(test_sentence)\n",
        "print(\"\\nPart-of-Speech Tags:\")\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yTjOs4c1Lb6",
        "outputId": "1fcaf351-c7e7-416b-ea53-1ae8617dd24e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "1: Tokenization is an essential part of natural language processing.\n",
            "2: It involves breaking down text into smaller units, such as sentences or words.\n",
            "3: This makes it easier to analyze and process text data.\n",
            "\n",
            "Words:\n",
            "['Tokenization', 'is', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'It', 'involves', 'breaking', 'down', 'text', 'into', 'smaller', 'units', ',', 'such', 'as', 'sentences', 'or', 'words', '.', 'This', 'makes', 'it', 'easier', 'to', 'analyze', 'and', 'process', 'text', 'data', '.']\n",
            "\n",
            "Cleaned Text:\n",
            "hello world welcome to nlp 101\n",
            "\n",
            "Extracted Emails:\n",
            "['support@example.com', 'sales@example.org']\n",
            "\n",
            "Webpage Title:\n",
            "Example Domain\n",
            "WordCloud saved as wordcloud.png\n",
            "\n",
            "Part-of-Speech Tags:\n",
            "[('NLP', 'PROPN'), ('is', 'AUX'), ('amazing', 'ADJ'), ('and', 'CCONJ'), ('fun', 'ADJ'), ('to', 'PART'), ('learn', 'VERB'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    }
  ]
}