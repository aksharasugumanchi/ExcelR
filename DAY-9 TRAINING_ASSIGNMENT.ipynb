{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbAy0uTug/6d4lFBbWiw7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksharasugumanchi/ExcelR/blob/main/DAY-9%20TRAINING_ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Cleaning Lowercasing Removing Special Characters - Coding Assignment\n",
        "Assignment:\n",
        "Write a Python function to clean a given text by removing special characters and converting it to lowercase. Test it with the input: 'Hello, World! Welcome to NLP 101.'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "euc8Hoybi7YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty2rGHPyqRls",
        "outputId": "cc4f8688-04c9-46a3-f77d-64991f2126ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "\n",
        "# Download necessary resources from NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the given text by removing special characters and converting to lowercase.\n",
        "    :param text: str, input text to clean\n",
        "    :return: str, cleaned text\n",
        "    \"\"\"\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
        "    return cleaned_text.lower()  # Convert to lowercase\n",
        "\n",
        "# Sample paragraph\n",
        "paragraph = \"Tokenization is an essential part of natural language processing. It involves breaking down text into smaller units, such as sentences or words. This makes it easier to analyze and process text data.\"\n",
        "\n",
        "# Tokenize into sentences\n",
        "sentences = sent_tokenize(paragraph)\n",
        "print(\"Sentences:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}: {sentence}\")\n",
        "\n",
        "# Tokenize into words\n",
        "words = word_tokenize(paragraph)\n",
        "print(\"\\nWords:\")\n",
        "print(words)\n",
        "\n",
        "# Test clean_text function\n",
        "test_input = 'Hello, World! Welcome to NLP 101.'\n",
        "cleaned_output = clean_text(test_input)\n",
        "print(\"\\nCleaned Text:\")\n",
        "print(cleaned_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmvfla07qwOx",
        "outputId": "6574b7db-4ec7-4731-ac99-44f07e9706ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "1: Tokenization is an essential part of natural language processing.\n",
            "2: It involves breaking down text into smaller units, such as sentences or words.\n",
            "3: This makes it easier to analyze and process text data.\n",
            "\n",
            "Words:\n",
            "['Tokenization', 'is', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'It', 'involves', 'breaking', 'down', 'text', 'into', 'smaller', 'units', ',', 'such', 'as', 'sentences', 'or', 'words', '.', 'This', 'makes', 'it', 'easier', 'to', 'analyze', 'and', 'process', 'text', 'data', '.']\n",
            "\n",
            "Cleaned Text:\n",
            "hello world welcome to nlp 101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}