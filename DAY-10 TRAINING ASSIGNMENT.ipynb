{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0nFP3cQRhEFtzUx6M1/Bi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksharasugumanchi/ExcelR/blob/main/DAY-10%20TRAINING%20ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python function using regular expressions to extract all email addresses from a given string. Test it with the input: 'Contact us at support@example.com and sales@example.org.'\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "euc8Hoybi7YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty2rGHPyqRls",
        "outputId": "cc4f8688-04c9-46a3-f77d-64991f2126ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "\n",
        "# Download necessary resources from NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the given text by removing special characters and converting to lowercase.\n",
        "    :param text: str, input text to clean\n",
        "    :return: str, cleaned text\n",
        "    \"\"\"\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
        "    return cleaned_text.lower()  # Convert to lowercase\n",
        "\n",
        "def extract_emails(text):\n",
        "    \"\"\"\n",
        "    Extracts all email addresses from the given text.\n",
        "    :param text: str, input text containing email addresses\n",
        "    :return: list, extracted email addresses\n",
        "    \"\"\"\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "    return re.findall(email_pattern, text)\n",
        "\n",
        "# Sample paragraph\n",
        "paragraph = \"Tokenization is an essential part of natural language processing. It involves breaking down text into smaller units, such as sentences or words. This makes it easier to analyze and process text data.\"\n",
        "\n",
        "# Tokenize into sentences\n",
        "sentences = sent_tokenize(paragraph)\n",
        "print(\"Sentences:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}: {sentence}\")\n",
        "\n",
        "# Tokenize into words\n",
        "words = word_tokenize(paragraph)\n",
        "print(\"\\nWords:\")\n",
        "print(words)\n",
        "\n",
        "# Test clean_text function\n",
        "test_input = 'Hello, World! Welcome to NLP 101.'\n",
        "cleaned_output = clean_text(test_input)\n",
        "print(\"\\nCleaned Text:\")\n",
        "print(cleaned_output)\n",
        "\n",
        "# Test extract_emails function\n",
        "test_email_input = 'Contact us at support@example.com and sales@example.org.'\n",
        "extracted_emails = extract_emails(test_email_input)\n",
        "print(\"\\nExtracted Emails:\")\n",
        "print(extracted_emails)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3HkrQvOy5tf",
        "outputId": "4f12bb94-a9a6-4cb0-bed4-784c95872756"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "1: Tokenization is an essential part of natural language processing.\n",
            "2: It involves breaking down text into smaller units, such as sentences or words.\n",
            "3: This makes it easier to analyze and process text data.\n",
            "\n",
            "Words:\n",
            "['Tokenization', 'is', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'It', 'involves', 'breaking', 'down', 'text', 'into', 'smaller', 'units', ',', 'such', 'as', 'sentences', 'or', 'words', '.', 'This', 'makes', 'it', 'easier', 'to', 'analyze', 'and', 'process', 'text', 'data', '.']\n",
            "\n",
            "Cleaned Text:\n",
            "hello world welcome to nlp 101\n",
            "\n",
            "Extracted Emails:\n",
            "['support@example.com', 'sales@example.org']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}